{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYVHcZa4DBSQMpwPSxr7Ej",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiiinchoi/MAI-Lab./blob/main/ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "abUz9AJb0TuI",
        "outputId": "c6fb3856-4180-4bc8-fcd1-12179b8bbf52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimage_size: 이미지의 크기\\npatch_size: 이미지를 패치로 나눌 크기\\nin_channels: 입력 이미지의 채널 수\\nnum_classes: 분류해야 하는 클래스 수\\nembed_dim: 패치 임베딩의 차원\\ndepth: 인코더 블록의 수\\nnum_heads: 멀티 헤드 어텐션의 헤드 수\\nmlp_ratio: MLP 모듈에서 첫 번째 FC 레이어와 두 번째 FC 레이어의 차원 비율 (기본값은 4.0)\\nqkv_bias: 어텐션의 행렬 연산에서 Q, K, V 행렬에 대한 바이어스 사용 여부 (기본값은 False)\\nattn_drop: 어텐션 드롭아웃 비율 (기본값은 0.0)\\nproj_drop: 어텐션 이후 프로젝션 드롭아웃 비율 (기본값은 0.0)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "'''\n",
        "image_size: 이미지의 크기\n",
        "patch_size: 이미지를 패치로 나눌 크기\n",
        "in_channels: 입력 이미지의 채널 수\n",
        "num_classes: 분류해야 하는 클래스 수\n",
        "embed_dim: 패치 임베딩의 차원\n",
        "depth: 인코더 블록의 수\n",
        "num_heads: 멀티 헤드 어텐션의 헤드 수\n",
        "mlp_ratio: MLP 모듈에서 첫 번째 FC 레이어와 두 번째 FC 레이어의 차원 비율 (기본값은 4.0)\n",
        "qkv_bias: 어텐션의 행렬 연산에서 Q, K, V 행렬에 대한 바이어스 사용 여부 (기본값은 False)\n",
        "attn_drop: 어텐션 드롭아웃 비율 (기본값은 0.0)\n",
        "proj_drop: 어텐션 이후 프로젝션 드롭아웃 비율 (기본값은 0.0)\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PatchEmbedding\n"
      ],
      "metadata": {
        "id": "RjFeM5ab1TAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지를 패치 단위 시퀀스로 바꾸는 Patch Embedding 모듈\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
        "        super(PatchEmbedding, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # (image_size/patch_size)^2 만큼 패치가 생김\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "        # patch_size랑 stride를 같게 해서 딱딱 맞게\n",
        "        self.projection = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.projection(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(1, 2)  # (배치, 패치 개수, 임베딩 차원)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Yy86AJOO1O1p"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP"
      ],
      "metadata": {
        "id": "-eCcSS2j1WWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer 블록 안에서 사용하는 두 개의 FC 레이어로 된 MLP 모듈\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, out_features, mlp_drop=0.):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.dropout = nn.Dropout(mlp_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "wKM4cYZL1O3w"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention"
      ],
      "metadata": {
        "id": "eKptThTG1bh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 멀티헤드 Self-Attention 모듈\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        # 한 번에 Q, K, V를 만들어주는 Linear\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim) # 멀티헤드 출력 합친 후 projection\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, N, C)  → B:배치, N:토큰 수(패치+CLS), C:채널(embed_dim)\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        # qkv: (3, B, num_heads, N, head_dim)\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        x = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        x = x.softmax(dim=-1)\n",
        "        x = self.attn_drop(x)\n",
        "\n",
        "        x = (x @ v).transpose(1, 2).reshape(B, N, C)\n",
        "\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "_T7hjR741O6V"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoderblock: Self-Attention + MLP + Residual"
      ],
      "metadata": {
        "id": "03CqJdTD1gTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ViT 인코더 블록 – LayerNorm + Self-Attention + MLP\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4.0, qkv_bias=False, attn_drop=0.0, proj_drop=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.attn = Attention(\n",
        "            dim=dim,\n",
        "            num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias,\n",
        "            attn_drop=attn_drop,\n",
        "            proj_drop=proj_drop\n",
        "        )\n",
        "        self.mlp = MLP(\n",
        "            in_features=dim,\n",
        "            hidden_features=int(dim * mlp_ratio),\n",
        "            out_features=dim\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "RP0E81Cd1O9Y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLPHead"
      ],
      "metadata": {
        "id": "O1_UhQqu1jg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CLS 토큰 벡터를 입력 받아 최종 클래스 점수를 출력하는 MLP 헤드 -> Transformer 인코더에서 나온 CLS 토큰 하나를 받아서, MLP 2층을 거쳐 최종 클래스 점수(logits)를 뽑는 부분\n",
        "\n",
        "class MLPHead(nn.Module):\n",
        "    def __init__(self, embed_dim, mlp_hidden_dim, num_classes):\n",
        "        super(MLPHead, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.mlp_hidden_dim = mlp_hidden_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.fc1 = nn.Linear(embed_dim, mlp_hidden_dim)\n",
        "        self.fc2 = nn.Linear(mlp_hidden_dim, mlp_hidden_dim)\n",
        "        self.fc3 = nn.Linear(mlp_hidden_dim, num_classes)\n",
        "        self.activation = nn.GELU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "lhOrZSKN1O_t"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT 모델"
      ],
      "metadata": {
        "id": "DgQdaKfZ1orJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ViT 모델 정의\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size,\n",
        "        patch_size,\n",
        "        in_channels,\n",
        "        num_classes,\n",
        "        embed_dim,\n",
        "        depth,\n",
        "        num_heads,\n",
        "        mlp_ratio=4.0,\n",
        "        qkv_bias=False,\n",
        "        attn_drop=0.0,\n",
        "        proj_drop=0.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1) 이미지 → 패치 시퀀스로 변환\n",
        "        self.patch_embed = PatchEmbedding(\n",
        "            image_size=image_size,\n",
        "            patch_size=patch_size,\n",
        "            in_channels=in_channels,\n",
        "            embed_dim=embed_dim\n",
        "        )\n",
        "        self.num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # 2) CLS 토큰 및 위치 임베딩 파라미터\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=proj_drop)\n",
        "\n",
        "        # 3) Encoder 블록 여러 개 쌓기\n",
        "        self.encoder_blocks = nn.ModuleList([\n",
        "            EncoderBlock(\n",
        "                dim=embed_dim,\n",
        "                num_heads=num_heads,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                attn_drop=attn_drop,\n",
        "                proj_drop=proj_drop\n",
        "            )\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        # 4) 최종 분류용 헤드\n",
        "        self.mlp_head = MLPHead(\n",
        "            embed_dim=embed_dim,\n",
        "            mlp_hidden_dim=embed_dim * 4,\n",
        "            num_classes=num_classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.patch_embed(x)\n",
        "        # 배치 크기에 맞게 CLS 토큰 복제\n",
        "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
        "\n",
        "        # 앞에 CLS 토큰 붙이기 → (B, N_patches+1, embed_dim)\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "\n",
        "        # 위치 임베딩 추가\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        # 인코더 블록을 depth만큼 순차적으로 통과\n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            x = encoder_block(x)\n",
        "\n",
        "        # 맨 앞 CLS 토큰만 뽑아서 분류 헤드로 전달\n",
        "        x = x[:, 0]\n",
        "        x = self.mlp_head(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "HFXWxaFY1PB0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViT(\n",
        "    image_size=224,\n",
        "    patch_size=16,\n",
        "    in_channels=3,\n",
        "    num_classes=4,\n",
        "    embed_dim=256,\n",
        "    depth=6,\n",
        "    num_heads=8,\n",
        ")\n",
        "\n",
        "x = torch.randn(2, 3, 224, 224)  # batch=2\n",
        "logits = model(x)\n",
        "print(logits.shape)  # torch.Size([2, 4])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNrca3760V0x",
        "outputId": "4df77e2d-7105-4f23-e881-9161fd9788d6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1YL9_6hG0V2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RdYw6_ir0V4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Dez-Usv0V6R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}